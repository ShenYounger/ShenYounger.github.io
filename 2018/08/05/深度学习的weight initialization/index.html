<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="本文转自知乎专栏文章聊一聊深度学习的weight initialization，作者为知乎用户夏飞。转载此篇文章的目的在于：在不修改原文内容的基础之上，添加个人的理解及思考心得。原文内容如下，小括号内的红色字体为本人所做的说明和心得。 TLDR (or the take-away)Weight Initialization matters!!! 深度学习中的weight initializatio">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习的weight initialization">
<meta property="og:url" content="http://yoursite.com/2018/08/05/深度学习的weight initialization/index.html">
<meta property="og:site_name" content="个人博客">
<meta property="og:description" content="本文转自知乎专栏文章聊一聊深度学习的weight initialization，作者为知乎用户夏飞。转载此篇文章的目的在于：在不修改原文内容的基础之上，添加个人的理解及思考心得。原文内容如下，小括号内的红色字体为本人所做的说明和心得。 TLDR (or the take-away)Weight Initialization matters!!! 深度学习中的weight initializatio">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-e70e859a530b59fa9a3fef0162dd8dfd_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-f87e8ef045f71385cfb9179b8a53cfa5_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-84e98522c9cf5e197e00351eb5b9ab35_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-c7bee22f2b2bbf671eecbbada800e876_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-b043f49670cac2c2c59e684a4fcf9d10_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-2b14851823a6ec035cc16147eb5e04b0_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-eb33bba58b2c722da42d8759d190d42a_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-8c50bceea5cad0cc2d90f2be7460301b_hd.jpg">
<meta property="og:updated_time" content="2018-08-06T15:05:14.071Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习的weight initialization">
<meta name="twitter:description" content="本文转自知乎专栏文章聊一聊深度学习的weight initialization，作者为知乎用户夏飞。转载此篇文章的目的在于：在不修改原文内容的基础之上，添加个人的理解及思考心得。原文内容如下，小括号内的红色字体为本人所做的说明和心得。 TLDR (or the take-away)Weight Initialization matters!!! 深度学习中的weight initializatio">
<meta name="twitter:image" content="https://pic4.zhimg.com/80/v2-e70e859a530b59fa9a3fef0162dd8dfd_hd.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/05/深度学习的weight initialization/"/>





  <title>深度学习的weight initialization | 个人博客</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/05/深度学习的weight initialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Younger">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习的weight initialization</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-05T13:42:42+08:00">
                2018-08-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文转自知乎专栏文章<strong>聊一聊深度学习的weight initialization</strong>，作者为知乎用户<a href="https://www.zhihu.com/people/feixia586" target="_blank" rel="noopener">夏飞</a>。转载此篇文章的目的在于：在不修改原文内容的基础之上，添加个人的理解及思考心得。原文内容如下，<strong>小括号内的红色字体</strong>为本人所做的说明和心得。</p>
<h2 id="TLDR-or-the-take-away"><a href="#TLDR-or-the-take-away" class="headerlink" title="TLDR (or the take-away)"></a><strong>TLDR (or the take-away)</strong></h2><p><strong>Weight Initialization matters!!!</strong> 深度学习中的weight initialization对模型收敛速度和模型质量有重要影响！</p>
<ul>
<li>在ReLU activation function中推荐使用Xavier Initialization的变种，暂且称之为He Initialization：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">W = np.random.randn(node_in, node_out) / np.sqrt(node_in / 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>使用Batch Normalization Layer可以有效降低深度网络对weight初始化的依赖：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># put this before nonlinear transformation</span><br><span class="line">layer = tf.contrib.layers.batch_norm(layer, center=True, scale=True,</span><br><span class="line">                                     is_training=True)</span><br></pre></td></tr></table></figure>
<p>实验代码请参见<a href="https://link.zhihu.com/?target=https%3A//github.com/feixia586/zhihu_material/tree/master/weight_initialization" target="_blank" rel="noopener">我的Github</a>。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>深度学习模型训练的过程本质是对weight（即参数 W）进行更新，这需要每个参数有相应的初始值。有人可能会说：“参数初始化有什么难点？直接将所有weight初始化为0或者初始化为随机数！” 对一些简单的机器学习模型，或当optimization function是convex function(<font color="red">优化函数为convex function时，无论初始值为多少，都能通过梯度下降的方法得到全局最优值</font>)时，这些简单的方法确实有效。然而对于深度学习而言，非线性函数被疯狂叠加，产生如本文题图所示的non-convex function，如何选择参数初始值便成为一个值得探讨的问题 —- 其本质是初始参数的选择应使得objective function便于被优化。事实上，在学术界这也是一个被actively研究的领域。</p>
<p>TLDR里已经涵盖了本文的核心要点，下面在正文中，我们来深入了解一下前因后果。</p>
<h2 id="初始化为0的可行性？"><a href="#初始化为0的可行性？" class="headerlink" title="初始化为0的可行性？"></a><strong>初始化为0的可行性？</strong></h2><p>答案是不可行。 这是一道送分题 哈哈！为什么将所有W初始化为0是错误的呢？是因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的 —- gradient相同，weight update也相同(<font color="red">严格说来，当输入层为多维并且每个维度的数值不一样时，输入层和第一个隐藏层之间的weight梯度并不是全部相等的。因为此时的weight梯度依赖于输入样本x在各个维度的数值</font>)。这显然是一个不可接受的结果。</p>
<h2 id="可行的几种初始化方式"><a href="#可行的几种初始化方式" class="headerlink" title="可行的几种初始化方式"></a><strong>可行的几种初始化方式</strong></h2><ul>
<li><strong>pre-training</strong></li>
</ul>
<p>pre-training是早期训练神经网络的有效初始化方法，一个便于理解的例子是先使用greedy layerwise auto-encoder做unsupervised pre-training，然后再做fine-tuning。具体过程可以参见UFLDL的一个<a href="https://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" target="_blank" rel="noopener">tutorial</a>，因为这不是本文重点，就在这里简略的说一下：（1）pre-training阶段，将神经网络中的每一层取出，构造一个auto-encoder做训练，使得输入层和输出层保持一致。在这一过程中，参数得以更新，形成初始值（2）fine-tuning阶段，将pre-train过的每一层放回神经网络，利用pre-train阶段得到的参数初始值和训练数据对模型进行整体调整。在这一过程中，参数进一步被更新，形成最终模型。</p>
<p>随着数据量的增加以及activation function (参见我的另一篇<a href="https://zhuanlan.zhihu.com/p/25110450" target="_blank" rel="noopener">文章</a>) 的发展，pre-training的概念已经渐渐发生变化。目前，从零开始训练神经网络时我们也很少采用auto-encoder进行pre-training，而是直奔主题做模型训练。不想从零开始训练神经网络时，我们往往选择一个已经训练好的在任务A上的模型（称为pre-trained model），将其放在任务B上做模型调整（称为fine-tuning）。</p>
<ul>
<li><strong>random initialization</strong></li>
</ul>
<p>随机初始化是很多人目前经常使用的方法，然而这是有弊端的，一旦随机分布选择不当，就会导致网络优化陷入困境。下面举几个例子。</p>
<p>核心代码见下方，完整代码请参见<a href="https://link.zhihu.com/?target=https%3A//github.com/feixia586/zhihu_material/tree/master/weight_initialization" target="_blank" rel="noopener">我的Github</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">data = tf.constant(np.random.randn(2000, 800))</span><br><span class="line">layer_sizes = [800 - 50 * i for i in range(0,10)]</span><br><span class="line">num_layers = len(layer_sizes)</span><br><span class="line"></span><br><span class="line">fcs = []  # To store fully connected layers&apos; output</span><br><span class="line">for i in range(0, num_layers - 1):</span><br><span class="line">    X = data if i == 0 else fcs[i - 1]</span><br><span class="line">    node_in = layer_sizes[i]</span><br><span class="line">    node_out = layer_sizes[i + 1]</span><br><span class="line">    W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01 </span><br><span class="line">    fc = tf.matmul(X, W)</span><br><span class="line">    fc = tf.nn.tanh(fc)</span><br><span class="line">    fcs.append(fc)</span><br></pre></td></tr></table></figure>
<p>这里我们创建了一个10层的神经网络，非线性变换为tanh，每一层的参数都是随机正态分布，均值为0，标准差为0.01。下图给出了每一层输出值分布的直方图。</p>
<p><img src="https://pic4.zhimg.com/80/v2-e70e859a530b59fa9a3fef0162dd8dfd_hd.jpg" alt="img"></p>
<p>随着层数的增加，我们看到输出值迅速向0靠拢，在后几层中，几乎所有的输出值$x$都很接近0！回忆优化神经网络的back propagation算法，根据链式法则，gradient等于当前函数的gradient乘以后一层的gradient，这意味着输出值$x$是计算gradient中的乘法因子，直接导致gradient很小，使得参数难以被更新！(<font color="red">不太理解作者的原意，不过从UFLDL关于反向传播的公式来看，假如反向传播的反向为从第i+1层到第i层，那么两层之间weight的更新依赖于第i层的输出，当激活函数为tanh时，第i层的输出都很接近0，这表明两层之间的weight无法更新</font>)</p>
<p>让我们将初始值调大一些：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out))</span><br></pre></td></tr></table></figure>
<p>均值仍然为0，标准差现在变为1，下图是每一层输出值分布的直方图：</p>
<p><img src="https://pic3.zhimg.com/80/v2-f87e8ef045f71385cfb9179b8a53cfa5_hd.jpg" alt="img"></p>
<p>几乎所有的值集中在-1或1附近，神经元saturated了！注意到tanh在-1和1附近的gradient都接近0，这同样导致了gradient太小，参数难以被更新。<font color="red">不过从UFLDL关于反向传播的公式来看，假如反向传播的反向为从第i+1层到第i层，那么两层之间weight的更新依赖于第i+1层的输出对于第i+1层输入的导数，当激活函数为tanh时，导数很接近0，这表明两层之间的weight同样无法更新</font></p>
<ul>
<li><strong>Xavier initialization</strong></li>
</ul>
<p>Xavier initialization可以解决上面的问题！其初始化方式也并不复杂。Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。注意，为了问题的简便，Xavier初始化的推导过程是基于线性函数的，但是它在一些非线性神经元中也很有效。让我们试一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out)) / np.sqrt(node_in)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic1.zhimg.com/80/v2-84e98522c9cf5e197e00351eb5b9ab35_hd.jpg" alt="img"></p>
<p>Woohoo！输出值在很多层之后依然保持着良好的分布，这很有利于我们优化神经网络！之前谈到Xavier initialization是在线性函数上推导得出，这说明它对非线性函数并不具有普适性，所以这个例子仅仅说明它对tanh很有效，那么对于目前最常用的ReLU神经元呢（关于不同非线性神经元的比较请参考<a href="https://zhuanlan.zhihu.com/p/25110450" target="_blank" rel="noopener">这里</a>）？继续做一下实验：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out)) / np.sqrt(node_in)</span><br><span class="line">......</span><br><span class="line">fc = tf.nn.relu(fc)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic4.zhimg.com/80/v2-c7bee22f2b2bbf671eecbbada800e876_hd.jpg" alt="img"></p>
<p>前面看起来还不错，后面的趋势却是越来越接近0。幸运的是，He initialization可以用来解决ReLU初始化的问题。</p>
<ul>
<li><strong>He initialization</strong></li>
</ul>
<p>He initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in,node_out)) / np.sqrt(node_in/2)</span><br><span class="line">......</span><br><span class="line">fc = tf.nn.relu(fc)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-b043f49670cac2c2c59e684a4fcf9d10_hd.jpg" alt="img"></p>
<p>看起来效果非常好，推荐在ReLU网络中使用！</p>
<h2 id="Batch-Normalization-Layer"><a href="#Batch-Normalization-Layer" class="headerlink" title="Batch Normalization Layer"></a><strong>Batch Normalization Layer</strong></h2><p>Batch Normalization是一种巧妙而粗暴的方法来削弱bad initialization的影响，其基本思想是：If you want it, just make it!</p>
<p>我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换：</p>
<p><img src="https://pic2.zhimg.com/80/v2-2b14851823a6ec035cc16147eb5e04b0_hd.jpg" alt="img"></p>
<p>Batch Normalization中所有的操作都是平滑可导，这使得back propagation可以有效运行并学到相应的参数$\gamma$，$\beta$。需要注意的一点是Batch Normalization在training和testing时行为有所差别。Training时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$由当前batch计算得出；在Testing时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$应使用Training时保存的均值或类似的经过处理的值，而不是由当前batch计算。</p>
<p>随机初始化，无Batch Normalization：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01</span><br><span class="line">......</span><br><span class="line">fc = tf.nn.relu(fc)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-eb33bba58b2c722da42d8759d190d42a_hd.jpg" alt="img"></p>
<p>随机初始化，有Batch Normalization：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(np.random.randn(node_in, node_out)) * 0.01</span><br><span class="line">......</span><br><span class="line">fc = tf.contrib.layers.batch_norm(fc, center=True, scale=True,</span><br><span class="line">                                  is_training=True)</span><br><span class="line">fc = tf.nn.relu(fc)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/80/v2-8c50bceea5cad0cc2d90f2be7460301b_hd.jpg" alt="img"></p>
<p>很容易看到，Batch Normalization的效果非常好，推荐使用！</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><p>Xavier initialization是由Xavier Glorot et al.在2010年提出，He initialization是由Kaiming He et al.在2015年提出，Batch Normalization是由Sergey Ioffe et al.在2015年提出。</p>
<p>另有知乎网友在评论中提到了一些其他相关工作：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.06422" target="_blank" rel="noopener">https://arxiv.org/abs/1511.06422</a>， <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1702.08591.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1702.08591.pdf</a></p>
<ol>
<li>Xavier Glorot et al., Understanding the Difficult of Training Deep Feedforward Neural Networks</li>
<li>Kaiming He et al., Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classfication</li>
<li>Sergey Ioffe et al., Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/04/神经网络参数初始化/" rel="next" title="神经网络参数初始化">
                <i class="fa fa-chevron-left"></i> 神经网络参数初始化
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Younger</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#TLDR-or-the-take-away"><span class="nav-number">1.</span> <span class="nav-text">TLDR (or the take-away)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">2.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始化为0的可行性？"><span class="nav-number">3.</span> <span class="nav-text">初始化为0的可行性？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#可行的几种初始化方式"><span class="nav-number">4.</span> <span class="nav-text">可行的几种初始化方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization-Layer"><span class="nav-number">5.</span> <span class="nav-text">Batch Normalization Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Younger</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
